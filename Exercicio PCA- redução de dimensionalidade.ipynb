{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introdução\n",
    "\n",
    "Estaremos usando dados de clientes de um [distribuidor atacadista português](https://archive.ics.uci.edu/ml/datasets/Wholesale+customers?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel- SkillsNetworkCoursesIBMML0187ENSkillsNetwork821-2023-01-01) para clustering. Este arquivo de dados é chamado `Wholesale_Customers_Data`.\n",
    "\n",
    "Ele contém os seguintes recursos:\n",
    "\n",
    "\n",
    "* Fresco: gasto anual (m.u.) em produtos frescos\n",
    "* Leite: gasto anual (m.u.) com produtos lácteos\n",
    "* Mercearia: gasto anual (m.u.) em produtos de mercearia\n",
    "* Congelados: gasto anual (m.u.) com produtos congelados\n",
    "* Detergentes_Papel: gasto anual (m.u.) em detergentes e produtos de papel\n",
    "* Delicatessen: gasto anual (m.u.) em produtos de delicatessen\n",
    "* Canal: canal cliente (1: hotel/restaurante/café ou 2: retalho)\n",
    "* Região: região do cliente (1: Lisboa, 2: Porto, 3: Outro)\n",
    "\n",
    "Nesses dados, os valores de todos os gastos são dados em uma unidade arbitrária (m.u. = unidade monetária)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "\n",
    "import seaborn as sns, pandas as pd, numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pandas as pd, numpy as np, seaborn as sns, matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 1\n",
    "\n",
    "Nesta seção, iremos:\n",
    "\n",
    "* Importe os dados e verifique os tipos de dados.\n",
    "* Elimine as colunas de canal e região, pois elas não serão usadas, pois nos concentramos em colunas numéricas para este exemplo.\n",
    "* Converta as colunas restantes em floats, se necessário.\n",
    "* Copie esta versão dos dados (usando o método `copy`) para uma variável para preservá-la. Nós o usaremos mais tarde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML0187EN-SkillsNetwork/labs/module%203/data/Wholesale_Customers_Data.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['Channel', 'Region'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converte os floats\n",
    "for col in data.columns:\n",
    "    data[col] = data[col].astype(np.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preservo o dataset original\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_orig = data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 2\n",
    "\n",
    "Assim como na lição anterior, precisamos garantir que os dados sejam dimensionados e (relativamente) distribuídos normalmente.\n",
    "\n",
    "* Examine a correlação e a inclinação.\n",
    "* Execute quaisquer transformações e dimensione dados usando seu método de dimensionamento favorito.\n",
    "* Veja os gráficos de correlação pairwise dos novos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "corr_mat = data.corr()\n",
    "\n",
    "for x in range(corr_mat.shape[0]):\n",
    "    corr_mat.iloc[x,x] = 0.0\n",
    "    \n",
    "corr_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Como antes, as duas categorias com suas respectivas variáveis ​​mais fortemente correlacionadas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "corr_mat.abs().idxmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine os valores de inclinação e a transformação de log. Parece que todos precisam.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "log_columns = data.skew().sort_values(ascending=False)\n",
    "log_columns = log_columns.loc[log_columns > 0.75]\n",
    "\n",
    "log_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in log_columns.index:\n",
    "    data[col] = np.log1p(data[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Escale os dados novamente. Vamos usar `MinMaxScaler` desta vez apenas para misturar as coisas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "mms = MinMaxScaler()\n",
    "\n",
    "for col in data.columns:\n",
    "    data[col] = mms.fit_transform(data[[col]]).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Visualize a relação entre as variáveis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "sns.set_context('notebook')\n",
    "sns.set_style('white')\n",
    "sns.pairplot(data);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 3\n",
    "\n",
    "Nesta seção, iremos:\n",
    "* Usando a [função de pipeline] do Scikit-learn (http://scikit-learn.org/stable/modules/pipeline.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187 ENSkillsNetwork821-2023-01- 01), recrie o esquema de pré-processamento de dados acima (transformação e dimensionamento) usando um pipeline. Se você usou uma função de aprendizado não Scikit para transformar os dados (por exemplo, a função de log do NumPy), verifique a classe de transformador personalizada chamada [`FunctionTransformer`](http://scikit-learn.org/stable/modules/preprocessing.html? utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork821-2023-01-01#custom-transformers).\n",
    "* Use o pipeline para transformar os dados originais que foram armazenados no final da pergunta 1.\n",
    "* Compare os resultados com os dados originais para verificar se tudo funcionou.\n",
    "\n",
    "*Observação:* Scikit-learn tem uma função `Pipeline` mais flexível e uma versão de atalho chamada `make_pipeline`. Qualquer um pode ser usado. Além disso, se diferentes transformações precisam ser executadas nos dados, um [`FeatureUnion`](http://scikit-learn.org/stable/modules/pipeline.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA- SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork821-2023-01-01#featureunion-composite-feature-spaces) podem ser usados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# O transformador de log NumPy personalizado\n",
    "log_transformer = FunctionTransformer(np.log1p)\n",
    "\n",
    "# O pipeline\n",
    "estimators = [('log1p', log_transformer), ('minmaxscale', MinMaxScaler())]\n",
    "pipeline = Pipeline(estimators)\n",
    "\n",
    "# Converter os dados originais\n",
    "data_pipe = pipeline.fit_transform(data_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "np.allclose(data_pipe, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 4\n",
    "\n",
    "Nesta seção, iremos:\n",
    "* Execute o PCA com `n_components` variando de 1 a 5.\n",
    "* Armazene a quantidade de variação explicada para cada número de dimensões.\n",
    "* Também armazene a importância do recurso para cada número de dimensões. *Dica:* O PCA não fornece isso explicitamente depois que um modelo é ajustado, mas as propriedades `components_` podem ser usadas para determinar algo que se aproxime da importância. Como você decidiu fazer isso depende inteiramente de você.\n",
    "* Plote a variância explicada e as importâncias dos recursos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca_list = list()\n",
    "feature_weight_list = list()\n",
    "\n",
    "# Adapta-se a uma variedade de modelos de PCA\n",
    "\n",
    "for n in range(1, 6):\n",
    "    \n",
    "    # Criar e ajustar o modelo\n",
    "    PCAmod = PCA(n_components=n)\n",
    "    PCAmod.fit(data)\n",
    "    \n",
    "    # Armazene o modelo e a variância\n",
    "    pca_list.append(pd.Series({'n':n, 'model':PCAmod,\n",
    "                               'var': PCAmod.explained_variance_ratio_.sum()}))\n",
    "    \n",
    "    # Calcule e armazene as importâncias dos recursos\n",
    "    abs_feature_values = np.abs(PCAmod.components_).sum(axis=0)\n",
    "    feature_weight_list.append(pd.DataFrame({'n':n, \n",
    "                                             'features': data.columns,\n",
    "                                             'values':abs_feature_values/abs_feature_values.sum()}))\n",
    "    \n",
    "pca_df = pd.concat(pca_list, axis=1).T.set_index('n')\n",
    "pca_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "features_df = (pd.concat(feature_weight_list)\n",
    "               .pivot(index='n', columns='features', values='values'))\n",
    "\n",
    "features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "sns.set_context('talk')\n",
    "ax = pca_df['var'].plot(kind='bar')\n",
    "\n",
    "ax.set(xlabel='Number of dimensions',\n",
    "       ylabel='Percent explained variance',\n",
    "       title='Explained Variance vs Dimensions');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "E aqui está um gráfico de importâncias de recursos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "ax = features_df.plot(kind='bar', figsize=(13,8))\n",
    "ax.legend(loc='upper right')\n",
    "ax.set(xlabel='Number of dimensions',\n",
    "       ylabel='Relative importance',\n",
    "       title='Feature importance vs Dimensions');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 5\n",
    "\n",
    "Nesta seção, iremos:\n",
    "* Ajuste um modelo `KernelPCA` com `kernel='rbf'`. Você pode escolher quantos componentes e quais valores usar para os outros parâmetros (`rbf` refere-se a um kernel de função de base radial, e o parâmetro `gamma` rege a escala desse kernel e normalmente varia entre 0 e 1). Vários outros [kernels](https://scikit-learn.org/stable/modules/metrics.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork821- 2023-01-01) pode ser Tentei e até passou os parâmetros de validação cruzada da SS (consulte este [exemplo] (https://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_digits.html?utm_exinfluencher&soarch_sourcegits. 555 & utm_id = na-skillsnetwork- Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork821-2023-01-01)).\n",
    "* Se você quiser mexer um pouco mais, use `GridSearchCV` para ajustar os parâmetros do modelo `KernelPCA`.\n",
    "\n",
    "A segunda etapa é complicada, pois as pesquisas em grade geralmente são usadas para métodos de aprendizado de máquina supervisionado e dependem de métricas de pontuação, como precisão, para determinar o melhor modelo. No entanto, uma função de pontuação personalizada pode ser escrita para `GridSearchCV`, onde maior é melhor para o resultado da função de pontuação.\n",
    "\n",
    "O que essa métrica envolveria para o PCA? E a porcentagem da variância explicada? Ou talvez o erro quadrático médio negativo nos dados depois de terem sido transformados e depois transformados inversamente?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Pontuador personalizado--use o rmse negativo da transformação inversa\n",
    "def scorer(pcamodel, X, y=None):\n",
    "\n",
    "    try:\n",
    "        X_val = X.values\n",
    "    except:\n",
    "        X_val = X\n",
    "        \n",
    "    # Calcular e transformar inversamente os dados\n",
    "    data_inv = pcamodel.fit(X_val).transform(X_val)\n",
    "    data_inv = pcamodel.inverse_transform(data_inv)\n",
    "    \n",
    "    # O cálculo do erro\n",
    "    mse = mean_squared_error(data_inv.ravel(), X_val.ravel())\n",
    "    \n",
    "    # Valores maiores são melhores para pontuadores, então tome valor negativo\n",
    "    return -1.0 * mse\n",
    "\n",
    "# Os parâmetros de pesquisa da grade\n",
    "param_grid = {'gamma':[0.001, 0.01, 0.05, 0.1, 0.5, 1.0],\n",
    "              'n_components': [2, 3, 4]}\n",
    "\n",
    "# A grade de pesquisa\n",
    "kernelPCA = GridSearchCV(KernelPCA(kernel='rbf', fit_inverse_transform=True),\n",
    "                         param_grid=param_grid,\n",
    "                         scoring=scorer,\n",
    "                         n_jobs=-1)\n",
    "\n",
    "\n",
    "kernelPCA = kernelPCA.fit(data)\n",
    "\n",
    "kernelPCA.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 6\n",
    "\n",
    "Vamos explorar como a precisão de nosso modelo pode mudar se incluirmos um `PCA` em nosso pipeline de construção de modelo. Vamos planejar usar a classe `Pipeline` do sklearn e criar um pipeline que tenha as seguintes etapas:\n",
    "<ol>\n",
    "  <li>Um escalador</li>\n",
    "  <li>`PCA(n_components=n)`</li>\n",
    "  <li>`Regressão Logística`</li>\n",
    "</ol>\n",
    "\n",
    "* Carregue os dados de Atividade Humana dos conjuntos de dados.\n",
    "* Escreva uma função que receba um valor de `n` e faça o pipeline acima, depois preveja a coluna \"Atividade\" em um StratifiedShuffleSplit 5 vezes e retorne a precisão média do teste\n",
    "* Para vários valores de n, chame a função acima e armazene as precisões médias.\n",
    "* Plote a precisão média por número de dimensões.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML0187EN-SkillsNetwork/Human_Activity_Recognition_Using_Smartphones_Data.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X = data.drop('Activity', axis=1)\n",
    "y = data.Activity\n",
    "sss = StratifiedShuffleSplit(n_splits=5, random_state=42)\n",
    "\n",
    "def get_avg_score(n):\n",
    "    pipe = [\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('pca', PCA(n_components=n)),\n",
    "        ('estimator', LogisticRegression(solver='liblinear'))\n",
    "    ]\n",
    "    pipe = Pipeline(pipe)\n",
    "    scores = []\n",
    "    for train_index, test_index in sss.split(X, y):\n",
    "        X_train, X_test = X.loc[train_index], X.loc[test_index]\n",
    "        y_train, y_test = y.loc[train_index], y.loc[test_index]\n",
    "        pipe.fit(X_train, y_train)\n",
    "        scores.append(accuracy_score(y_test, pipe.predict(X_test)))\n",
    "    return np.mean(scores)\n",
    "\n",
    "\n",
    "ns = [10, 20, 50, 100, 150, 200, 300, 400]\n",
    "score_list = [get_avg_score(n) for n in ns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "sns.set_context('talk')\n",
    "\n",
    "ax = plt.axes()\n",
    "ax.plot(ns, score_list)\n",
    "ax.set(xlabel='Number of Dimensions',\n",
    "       ylabel='Average Accuracy',\n",
    "       title='LogisticRegression Accuracy vs Number of dimensions on the Human Activity Dataset')\n",
    "ax.grid(True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
